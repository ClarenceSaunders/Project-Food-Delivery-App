import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import (
    accuracy_score, 
    precision_score, 
    recall_score, 
    f1_score,
    confusion_matrix,
    classification_report,
    roc_auc_score,
    roc_curve
)
import warnings
warnings.filterwarnings('ignore')

df = pd.read_csv('food_order_cleaned.csv', index_col = 0)
df

df.columns

# ============================================================================
# 2. CREATE TARGET VARIABLE
# ============================================================================

print("\n" + "=" * 80)
print("PREPARING TARGET VARIABLE")
print("=" * 80)

# Create binary target: On-Time Delivery (1 if delivery_time <= median, 0 otherwise)
median_delivery_time = df['delivery_time'].median()
df['on_time_delivery'] = (df['delivery_time'] <= median_delivery_time).astype(int)

print(f"\nTarget Variable Created: 'on_time_delivery'")
print(f"  - Median Delivery Time: {median_delivery_time:.2f} minutes")
print(f"  - On-Time (1): {(df['on_time_delivery'] == 1).sum()} orders ({(df['on_time_delivery'] == 1).sum() / len(df) * 100:.1f}%)")
print(f"  - Late (0): {(df['on_time_delivery'] == 0).sum()} orders ({(df['on_time_delivery'] == 0).sum() / len(df) * 100:.1f}%)")

# ============================================================================
# 3. FEATURE ENGINEERING
# ============================================================================

print("\n" + "=" * 80)
print("FEATURE ENGINEERING")
print("=" * 80)

# Select numeric features for model
numeric_features = ['cost_of_the_order', 'food_preparation_time', 'delivery_time']

# Create additional features
df['total_service_time'] = df['food_preparation_time'] + df['delivery_time']
df['order_value_category'] = pd.cut(df['cost_of_the_order'], bins=3, labels=['Low', 'Medium', 'High'])
df['prep_time_category'] = pd.cut(df['food_preparation_time'], bins=3, labels=['Fast', 'Medium', 'Slow'])

# Encode categorical variables
le_day = LabelEncoder()
df['day_of_week_encoded'] = le_day.fit_transform(df['day_of_the_week'])

le_cuisine = LabelEncoder()
df['cuisine_type_encoded'] = le_cuisine.fit_transform(df['cuisine_type'])

# Final feature set
features = [
    'cost_of_the_order',
    'food_preparation_time',
    'day_of_week_encoded',
    'cuisine_type_encoded',
    'total_service_time'
]

# Remove rows with missing values in target or features
df_model = df[features + ['on_time_delivery']].dropna()

print(f"\nFeatures Selected: {len(features)}")
for i, feat in enumerate(features, 1):
    print(f"  {i}. {feat}")

print(f"\nFinal Dataset Size: {df_model.shape[0]} rows (after removing missing values)")

X = df_model[features]
y = df_model['on_time_delivery']

print(f"\nTarget Distribution:")
print(f"  - On-Time: {(y == 1).sum()} ({(y == 1).sum() / len(y) * 100:.1f}%)")
print(f"  - Late: {(y == 0).sum()} ({(y == 0).sum() / len(y) * 100:.1f}%)")

# ============================================================================
# 4. EXPLORATORY DATA ANALYSIS
# ============================================================================

print("\n" + "=" * 80)
print("EXPLORATORY DATA ANALYSIS")
print("=" * 80)

print(f"\nFeature Statistics:")
print(X.describe())

print(f"\nCorrelation with Target:")
correlation_with_target = X.copy()
correlation_with_target['on_time_delivery'] = y
print(correlation_with_target.corr()['on_time_delivery'].sort_values(ascending=False))

# ============================================================================
# 5. TRAIN/TEST SPLIT
# ============================================================================

print("\n" + "=" * 80)
print("TRAIN/TEST SPLIT")
print("=" * 80)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)
print(f"\n✓ Train/Test Split (Stratified):")
print(f"  - Training set: {X_train.shape[0]} records")
print(f"  - Test set: {X_test.shape[0]} records")
print(f"  - Training On-Time: {(y_train == 1).sum()} ({(y_train == 1).sum() / len(y_train) * 100:.1f}%)")
print(f"  - Testing On-Time: {(y_test == 1).sum()} ({(y_test == 1).sum() / len(y_test) * 100:.1f}%)")

# ============================================================================
# 6. FEATURE SCALING
# ============================================================================
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
print(f"\n✓ Features scaled using StandardScaler")

# ============================================================================
# 7. BUILD CLASSIFICATION MODELS
# ============================================================================

print("\n" + "=" * 80)
print("TRAINING CLASSIFICATION MODELS")
print("=" * 80)

models = {
    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),
    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42)
}

results = {}

for model_name, model in models.items():
    print(f"\n{'─' * 80}")
    print(f"Training: {model_name}")
    print(f"{'─' * 80}")
    
    # Train model
    model.fit(X_train_scaled, y_train)
    
    # Make predictions
    y_pred_train = model.predict(X_train_scaled)
    y_pred_test = model.predict(X_test_scaled)
    
    # Probability predictions for ROC-AUC
    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]
    
    # Calculate metrics
    train_accuracy = accuracy_score(y_train, y_pred_train)
    test_accuracy = accuracy_score(y_test, y_pred_test)
    precision = precision_score(y_test, y_pred_test)
    recall = recall_score(y_test, y_pred_test)
    f1 = f1_score(y_test, y_pred_test)
    roc_auc = roc_auc_score(y_test, y_pred_proba)
    
    # Cross-validation score
    cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='accuracy')
    
    # Store results
    results[model_name] = {
        'model': model,
        'train_accuracy': train_accuracy,
        'test_accuracy': test_accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'roc_auc': roc_auc,
        'cv_mean': cv_scores.mean(),
        'cv_std': cv_scores.std(),
        'y_pred': y_pred_test,
        'y_pred_proba': y_pred_proba
    }
    
    # Print results
    print(f"Training Accuracy:     {train_accuracy:.4f}")
    print(f"Testing Accuracy:      {test_accuracy:.4f}")
    print(f"Precision:             {precision:.4f}")
    print(f"Recall:                {recall:.4f}")
    print(f"F1-Score:              {f1:.4f}")
    print(f"ROC-AUC Score:         {roc_auc:.4f}")
    print(f"Cross-Val (Mean ± Std): {cv_scores.mean():.4f} ± {cv_scores.std():.4f}")

# ============================================================================
# 8. MODEL COMPARISON & SELECTION
# ============================================================================

print("\n" + "=" * 80)
print("MODEL PERFORMANCE COMPARISON")
print("=" * 80)

comparison_df = pd.DataFrame({
    'Model': results.keys(),
    'Train Accuracy': [results[m]['train_accuracy'] for m in results],
    'Test Accuracy': [results[m]['test_accuracy'] for m in results],
    'Precision': [results[m]['precision'] for m in results],
    'Recall': [results[m]['recall'] for m in results],
    'F1-Score': [results[m]['f1'] for m in results],
    'ROC-AUC': [results[m]['roc_auc'] for m in results]
})

print("\n", comparison_df.to_string(index=False))

# Select best model based on test accuracy
best_model_name = max(results, key=lambda x: results[x]['test_accuracy'])
best_model = results[best_model_name]['model']

print(f"\n✓ Best Model Selected: {best_model_name}")
print(f"  - Test Accuracy: {results[best_model_name]['test_accuracy']:.4f}")

# ============================================================================
# 9. DETAILED ANALYSIS OF BEST MODEL
# ============================================================================

print("\n" + "=" * 80)
print(f"DETAILED ANALYSIS: {best_model_name}")
print("=" * 80)

y_pred_best = results[best_model_name]['y_pred']

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred_best)
print("\nConfusion Matrix:")
print(cm)
print("\nInterpretation:")
print(f"  - True Negatives (Late predicted Late):   {cm[0, 0]}")
print(f"  - False Positives (Late predicted On-Time): {cm[0, 1]}")
print(f"  - False Negatives (On-Time predicted Late): {cm[1, 0]}")
print(f"  - True Positives (On-Time predicted On-Time): {cm[1, 1]}")

# Classification Report
print("\nClassification Report:")
print(classification_report(y_test, y_pred_best, target_names=['Late', 'On-Time']))

# Feature Importance (for tree-based models)
if best_model_name in ['Random Forest', 'Gradient Boosting']:
    feature_importance = pd.DataFrame({
        'Feature': features,
        'Importance': best_model.feature_importances_
    }).sort_values('Importance', ascending=False)
    
    print("\nFeature Importance:")
    print(feature_importance.to_string(index=False))

# ============================================================================
# 10. VISUALIZATIONS
# ============================================================================

fig, axes = plt.subplots(2, 2, figsize=(14, 10))
fig.suptitle('Food Delivery App - Classification Model Results', fontsize=16, fontweight='bold')

# 1. Model Comparison - Accuracy
ax1 = axes[0, 0]
model_names = list(results.keys())
test_accuracies = [results[m]['test_accuracy'] for m in model_names]
colors = ['#2ecc71' if m == best_model_name else '#3498db' for m in model_names]
ax1.bar(model_names, test_accuracies, color=colors)
ax1.set_ylabel('Accuracy', fontsize=11)
ax1.set_title('Model Comparison - Test Accuracy', fontweight='bold')
ax1.set_ylim([0, 1])
ax1.axhline(y=0.5, color='red', linestyle='--', alpha=0.5, label='Baseline (50%)')
for i, v in enumerate(test_accuracies):
    ax1.text(i, v + 0.02, f'{v:.3f}', ha='center', va='bottom', fontweight='bold')
ax1.legend()
ax1.grid(axis='y', alpha=0.3)

# 2. Confusion Matrix - Best Model
ax2 = axes[0, 1]
sns.heatmap(cm, annot=True, fmt='d', cmap='RdYlGn', ax=ax2, cbar=False, 
            xticklabels=['Late', 'On-Time'], yticklabels=['Late', 'On-Time'])
ax2.set_xlabel('Predicted', fontsize=11)
ax2.set_ylabel('Actual', fontsize=11)
ax2.set_title(f'Confusion Matrix - {best_model_name}', fontweight='bold')

# 3. ROC Curves
ax3 = axes[1, 0]
for model_name in results:
    fpr, tpr, _ = roc_curve(y_test, results[model_name]['y_pred_proba'])
    auc = results[model_name]['roc_auc']
    color = '#2ecc71' if model_name == best_model_name else '#3498db'
    ax3.plot(fpr, tpr, label=f'{model_name} (AUC={auc:.3f})', linewidth=2, color=color)
ax3.plot([0, 1], [0, 1], 'k--', label='Random Classifier', linewidth=1)
ax3.set_xlabel('False Positive Rate', fontsize=11)
ax3.set_ylabel('True Positive Rate', fontsize=11)
ax3.set_title('ROC Curves - All Models', fontweight='bold')
ax3.legend(loc='lower right')
ax3.grid(True, alpha=0.3)

# 4. Feature Importance or Metrics Comparison
ax4 = axes[1, 1]
if best_model_name in ['Random Forest', 'Gradient Boosting']:
    top_features = feature_importance.head(len(features))
    ax4.barh(range(len(top_features)), top_features['Importance'].values, color='#e74c3c')
    ax4.set_yticks(range(len(top_features)))
    ax4.set_yticklabels(top_features['Feature'].values, fontsize=10)
    ax4.set_xlabel('Importance', fontsize=11)
    ax4.set_title(f'Feature Importance - {best_model_name}', fontweight='bold')
    ax4.invert_yaxis()
else:
    # Show performance metrics for Logistic Regression
    metrics_names = ['Precision', 'Recall', 'F1-Score']
    metrics_values = [
        results[best_model_name]['precision'],
        results[best_model_name]['recall'],
        results[best_model_name]['f1']
    ]
    colors_metrics = ['#3498db', '#2ecc71', '#e74c3c']
    ax4.bar(metrics_names, metrics_values, color=colors_metrics)
    ax4.set_ylabel('Score', fontsize=11)
    ax4.set_title(f'Model Metrics - {best_model_name}', fontweight='bold')
    ax4.set_ylim([0, 1])
    for i, v in enumerate(metrics_values):
        ax4.text(i, v + 0.02, f'{v:.3f}', ha='center', va='bottom', fontweight='bold')
    ax4.grid(axis='y', alpha=0.3)

plt.tight_layout()
plt.savefig('food_delivery_classification_results.png', dpi=300, bbox_inches='tight')
print("\n✓ Visualizations saved to 'food_delivery_classification_results.png'")
plt.show()

# ============================================================================
# 11. SUMMARY REPORT
# ============================================================================

print("\n" + "=" * 80)
print("CLASSIFICATION MODEL - SUMMARY REPORT")
print("=" * 80)

print(f"\nDataset Information:")
print(f"  - Total Food Orders: {df_model.shape[0]:,}")
print(f"  - Features Used: {len(features)}")
print(f"  - Training Set: {X_train.shape[0]:,} orders")
print(f"  - Test Set: {X_test.shape[0]:,} orders")

print(f"\nTarget Variable Distribution:")
print(f"  - On-Time Deliveries: {(y == 1).sum():,} ({(y == 1).sum() / len(y) * 100:.1f}%)")
print(f"  - Late Deliveries: {(y == 0).sum():,} ({(y == 0).sum() / len(y) * 100:.1f}%)")

print(f"\nBest Performing Model: {best_model_name}")
print(f"  - Test Accuracy: {results[best_model_name]['test_accuracy']:.4f}")
print(f"  - Precision: {results[best_model_name]['precision']:.4f}")
print(f"  - Recall: {results[best_model_name]['recall']:.4f}")
print(f"  - F1-Score: {results[best_model_name]['f1']:.4f}")
print(f"  - ROC-AUC: {results[best_model_name]['roc_auc']:.4f}")

print(f"\nKey Insights:")
print(f"  1. Median Delivery Time: {median_delivery_time:.2f} minutes")
avg_delivery_time = df['delivery_time'].mean()
print(f"  2. Average Delivery Time: {avg_delivery_time:.2f} minutes")
print(f"  3. Average Food Prep Time: {df['food_preparation_time'].mean():.2f} minutes")
print(f"  4. Most Common Cuisine: {df['cuisine_type'].mode()[0]}")
print(f"  5. Average Order Value: ${df['cost_of_the_order'].mean():.2f}")

print(f"\nModel Deployment Recommendations:")
print(f"  1. Deploy {best_model_name} for production")
print(f"  2. Monitor model performance on new deliveries weekly")
print(f"  3. Retrain model monthly with new delivery data")
print(f"  4. Use model to identify high-risk late deliveries")
print(f"  5. Consider ensemble methods for improved robustness")

print("\n" + "=" * 80)
print("✓ Classification model training completed successfully!")
print("=" * 80)
